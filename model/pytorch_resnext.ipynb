{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "목차",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "349.091px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "model_taejin.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lrux-1g2R6C",
        "outputId": "8633c6a6-13a8-4664-9aa0-17bd561641ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdGdO7wi2R6H",
        "outputId": "f088ce77-a1b2-4606-ac97-67d59f0af676"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print('pytorch version: {}'.format(torch.__version__))\n",
        "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pytorch version: 1.9.0+cu102\n",
            "GPU 사용 가능 여부: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExQ9nUhR3TR-"
      },
      "source": [
        "df = pd.read_csv('./drive/MyDrive/포토 뮤직리스트/image_labels/emo_labeled.csv', converters={'emotion':eval}).iloc[:, 1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsJuuhWA9ydv"
      },
      "source": [
        "gray = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "  try:\n",
        "    if c(Image.open(df['fig path'][i])).size()[0] != 3:\n",
        "      gray.append(i)\n",
        "  except:\n",
        "    print(i)\n",
        "    gray.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHq9ge0YCtw4",
        "outputId": "317db1bf-0923-405c-e2a5-a71954068454"
      },
      "source": [
        "print(gray)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[596, 2099, 3565, 3685, 3910, 4262, 5845, 6417]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyuZuJQMUQtd",
        "outputId": "0a8d3df8-2b28-427d-a4f3-30a50b7f72e7"
      },
      "source": [
        "gray"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Ay6QstTbz5"
      },
      "source": [
        "df = df.drop(gray)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEu15eEEU0HH"
      },
      "source": [
        "df.to_csv('./drive/MyDrive/포토 뮤직리스트/image_labels/emo_labeled_color.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZYn975X6CgA"
      },
      "source": [
        "train, test = train_test_split(df, test_size=0.2, random_state=23)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "MvQWi7ov6KNm",
        "outputId": "193a79d4-149b-4c46-dfff-d92a294c53c1"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fig path</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6802</th>\n",
              "      <td>/content/drive/MyDrive/포토 뮤직리스트/images...</td>\n",
              "      <td>[0, 0, 1, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4719</th>\n",
              "      <td>/content/drive/MyDrive/포토 뮤직리스트/love/u...</td>\n",
              "      <td>[0, 0, 1, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>578</th>\n",
              "      <td>/content/drive/MyDrive/포토 뮤직리스트/images...</td>\n",
              "      <td>[0, 0, 0, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1188</th>\n",
              "      <td>/content/drive/MyDrive/포토 뮤직리스트/images...</td>\n",
              "      <td>[0, 1, 0, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4669</th>\n",
              "      <td>/content/drive/MyDrive/포토 뮤직리스트/love/u...</td>\n",
              "      <td>[1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>/content/drive/MyDrive/포토 뮤직리스트/images...</td>\n",
              "      <td>[0, 1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>/content/drive/MyDrive/포토 뮤직리스트/images...</td>\n",
              "      <td>[0, 0, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6182</th>\n",
              "      <td>/content/drive/MyDrive/포토 뮤직리스트/images/1_resiz...</td>\n",
              "      <td>[0, 1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9264</th>\n",
              "      <td>/content/drive/MyDrive/포토 뮤직리스트/images...</td>\n",
              "      <td>[0, 1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8795</th>\n",
              "      <td>/content/drive/MyDrive/포토 뮤직리스트/images...</td>\n",
              "      <td>[0, 1, 0, 1, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7437 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               fig path          emotion\n",
              "6802  /content/drive/MyDrive/포토 뮤직리스트/images...  [0, 0, 1, 0, 1]\n",
              "4719  /content/drive/MyDrive/포토 뮤직리스트/love/u...  [0, 0, 1, 1, 0]\n",
              "578   /content/drive/MyDrive/포토 뮤직리스트/images...  [0, 0, 0, 1, 0]\n",
              "1188  /content/drive/MyDrive/포토 뮤직리스트/images...  [0, 1, 0, 1, 0]\n",
              "4669  /content/drive/MyDrive/포토 뮤직리스트/love/u...  [1, 0, 0, 0, 0]\n",
              "...                                                 ...              ...\n",
              "39    /content/drive/MyDrive/포토 뮤직리스트/images...  [0, 1, 0, 0, 0]\n",
              "347   /content/drive/MyDrive/포토 뮤직리스트/images...  [0, 0, 0, 0, 1]\n",
              "6182  /content/drive/MyDrive/포토 뮤직리스트/images/1_resiz...  [0, 1, 0, 0, 0]\n",
              "9264  /content/drive/MyDrive/포토 뮤직리스트/images...  [0, 1, 0, 0, 0]\n",
              "8795  /content/drive/MyDrive/포토 뮤직리스트/images...  [0, 1, 0, 1, 0]\n",
              "\n",
              "[7437 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--X33bur2R6K"
      },
      "source": [
        "class EmoImageDataset(Dataset):\n",
        "    def __init__(self, mode, transform=None):\n",
        "        if mode == 'train':\n",
        "          self.all_data = list(train['fig path'].values)\n",
        "          self.labels = list(train['emotion'].values)\n",
        "        else:\n",
        "          self.all_data = list(test['fig path'].values)\n",
        "          self.labels = list(test['emotion'].values)\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        data_path = self.all_data[index]\n",
        "        img = Image.open(data_path)\n",
        "        if self.transform != None:\n",
        "          img = self.transform(img)\n",
        "        \n",
        "        label = np.array(self.labels[index])\n",
        "        return [img, label]\n",
        "    \n",
        "    def __len__(self):\n",
        "        length = len(self.all_data)\n",
        "        return length\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qRsLQ1i7P1v"
      },
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "num_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGOjUKAh2R6L"
      },
      "source": [
        "transform =  transforms.Compose([\n",
        "        transforms.Resize([256, 256]),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "train_data = EmoImageDataset(mode='train', transform=transform)\n",
        "test_data = EmoImageDataset(mode='test', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNcAlCVD7eke"
      },
      "source": [
        "class Resnext50(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        resnet = models.resnext50_32x4d(pretrained=True)\n",
        "        resnet.fc = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=resnet.fc.in_features, out_features=n_classes)\n",
        "        )\n",
        "        self.base_model = resnet\n",
        "        self.sigm = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigm(self.base_model(x))\n",
        "\n",
        "\n",
        "model = Resnext50(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gx1JNmr9Yp7"
      },
      "source": [
        "criterion = nn.BCELoss().to(device) \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
        "\n",
        "model = model.to(device).double()\n",
        "val_every = 1\n",
        "saved_dir = './drive/MyDrive/포토 뮤직리스트/Modelling/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmOl80_Yk3Ng"
      },
      "source": [
        "temp1 = torch.Tensor([[1., 0., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [1., 1., 0., 1., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 0., 0., 0., 1.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [1., 0., 0., 0., 0.],\n",
        "        [0., 0., 1., 0., 1.],\n",
        "        [0., 1., 0., 1., 0.],\n",
        "        [0., 1., 0., 1., 0.],\n",
        "        [0., 1., 0., 1., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 0., 1., 1., 0.],\n",
        "        [1., 1., 0., 1., 0.],\n",
        "        [1., 1., 0., 1., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [1., 0., 0., 1., 0.],\n",
        "        [0., 0., 1., 1., 0.],\n",
        "        [0., 0., 1., 0., 1.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 0., 0., 1., 0.]])\n",
        "temp2 = torch.Tensor([[0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 1., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 1., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 1., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.]])"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y6UiXrYhCg-",
        "outputId": "f55732de-852c-462e-8596-f987becfe490"
      },
      "source": [
        "temp1 == temp2"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False, False,  True,  True,  True],\n",
              "        [ True,  True,  True,  True,  True],\n",
              "        [ True, False,  True, False,  True],\n",
              "        [ True,  True,  True,  True,  True],\n",
              "        [False,  True,  True, False,  True],\n",
              "        [ True, False,  True, False,  True],\n",
              "        [ True, False,  True, False, False],\n",
              "        [ True,  True,  True,  True,  True],\n",
              "        [ True, False,  True, False,  True],\n",
              "        [False, False,  True,  True,  True],\n",
              "        [ True, False, False, False, False],\n",
              "        [ True,  True,  True, False,  True],\n",
              "        [ True,  True,  True, False,  True],\n",
              "        [ True,  True,  True, False,  True],\n",
              "        [ True, False,  True, False,  True],\n",
              "        [ True,  True,  True,  True,  True],\n",
              "        [ True, False,  True, False,  True],\n",
              "        [ True,  True,  True,  True,  True],\n",
              "        [ True,  True, False,  True,  True],\n",
              "        [False, False,  True,  True,  True],\n",
              "        [False, False,  True,  True,  True],\n",
              "        [ True,  True,  True,  True,  True],\n",
              "        [ True,  True,  True,  True,  True],\n",
              "        [ True,  True,  True,  True,  True],\n",
              "        [ True, False,  True,  True,  True],\n",
              "        [ True,  True,  True,  True,  True],\n",
              "        [ True, False,  True, False,  True],\n",
              "        [False, False,  True, False,  True],\n",
              "        [ True, False, False, False,  True],\n",
              "        [ True, False, False,  True, False],\n",
              "        [ True, False,  True, False,  True],\n",
              "        [ True, False,  True, False,  True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIW2JACJHZEp"
      },
      "source": [
        "def validation(epoch, model, data_loader, criterion, device):\n",
        "    print('Start validation #{}'.format(epoch) )\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        total_loss = 0\n",
        "        cnt = 0\n",
        "        for i, (imgs, labels) in enumerate(data_loader):\n",
        "            imgs, labels = imgs.to(device).double(), labels.to(device).double()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total += imgs.size(0) * 5\n",
        "            argmax = torch.round(outputs)\n",
        "            temp1 = labels\n",
        "            temp2 = argmax\n",
        "            correct += (labels == argmax).sum().item()\n",
        "            total_loss += loss\n",
        "            cnt += 1\n",
        "        avrg_loss = total_loss / cnt\n",
        "        print('Validation #{}  Accuracy: {:.2f}%  Average Loss: {:.4f}'.format(epoch, correct / total * 100, avrg_loss))\n",
        "    model.train()\n",
        "    return avrg_loss\n",
        "\n",
        "def save_model(model, saved_dir, file_name='best_model.pt'):\n",
        "    os.makedirs(saved_dir, exist_ok=True)\n",
        "    check_point = {\n",
        "        'net': model.state_dict()\n",
        "    }\n",
        "    output_path = os.path.join(saved_dir, file_name)\n",
        "    torch.save(check_point, output_path)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnCQ07ep-zfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7c79d4-d938-4e27-91f0-2dde14d43282"
      },
      "source": [
        "print('Start training..')\n",
        "best_loss = 9999999\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (imgs, labels) in enumerate(train_loader):\n",
        "        imgs, labels = imgs.to(device).double(), labels.to(device).double()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() \n",
        "        optimizer.step()\n",
        "        argmax = torch.round(outputs)\n",
        "        accuracy = (labels == argmax).float().mean()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
        "                epoch+1, num_epochs, i+1, len(train_loader), loss.item(), accuracy.item() * 100))\n",
        "\n",
        "    if (epoch + 1) % val_every == 0:\n",
        "        avrg_loss = validation(epoch + 1, model, test_loader, criterion, device)\n",
        "        if avrg_loss < best_loss:\n",
        "            print('Best performance at epoch: {}'.format(epoch + 1))\n",
        "            print('Save model in', saved_dir)\n",
        "            best_loss = avrg_loss\n",
        "            save_model(model, saved_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "RFNQ0h5NDv9H",
        "outputId": "2e89ca46-9d5f-4e12-93b1-fa94a8dbc35b"
      },
      "source": [
        "for i, (data, labels) in enumerate(train_loader):\n",
        "  print(i)\n",
        "  print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "[tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0]), tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]), tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1]), tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])]\n",
            "1\n",
            "[tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0]), tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-15868cacc9e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-103-24b4ebf46da1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2850\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cSDnRcbUo-S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}